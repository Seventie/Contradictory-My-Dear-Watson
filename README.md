# Multilingual NLI (Merged + Enhanced)

This repository now contains a single notebook:

- `nli_merged_enhanced.ipynb`

It solves the Kaggle **Contradictory, My Dear Watson** task (multilingual Natural Language Inference).

## Dataset

Source: Kaggle competition dataset `contradictory-my-dear-watson`.

- Task: classify relation between `premise` and `hypothesis`
- Labels:
  - `0`: Entailment
  - `1`: Neutral
  - `2`: Contradiction
- Languages: 15 multilingual languages
- Main files used in notebook:
  - `train.csv` (includes labels)
  - `test.csv` (for inference)
  - `sample_submission.csv`

Expected Kaggle input paths:

- `../input/contradictory-my-dear-watson/train.csv`
- `../input/contradictory-my-dear-watson/test.csv`
- `../input/contradictory-my-dear-watson/sample_submission.csv`

## Notebook Flow (Vertical)

```mermaid
flowchart TB
    F1["Load Libraries and Config"] --> F2["Detect TPU or GPU Strategy"]
    F2 --> F3["Load Train and Test Data"]
    F3 --> F4["EDA: Language and Label Distribution"]
    F4 --> F5["Normalize Text"]
    F5 --> F6["Language+Label Stratified Split"]
    F6 --> F7["Tokenize Premise and Hypothesis Pairs"]
    F7 --> F8["Build tf.data Datasets"]
    F8 --> F9["Build Enhanced XLM-R Model"]
    F9 --> F10["Stage 1 Training: Frozen Backbone"]
    F10 --> F11["Stage 2 Training: Unfrozen Fine-Tuning"]
    F11 --> F12["Validation Audit and Per-Language Metrics"]
    F12 --> F13["Predict Test Set"]
    F13 --> F14["Write submission.csv"]
```

## Stack Used

- Python 3
- TensorFlow / Keras
- Hugging Face Transformers
- scikit-learn
- NumPy
- pandas
- Plotly

## Model Used

- Backbone: `joeddav/xlm-roberta-large-xnli`
- Head: masked mean pooling + dropout + dense softmax classifier
- Output classes: 3 (`entailment`, `neutral`, `contradiction`)

## Techniques Used

- Transfer learning with multilingual transformer
- Premise-hypothesis pair encoding
- Text normalization (`NFKC`, whitespace cleanup)
- Language+label stratified validation split
- Masked mean pooling
- Dropout regularization
- Class weights for training balance
- Label smoothing
- AdamW optimizer
- Cosine learning-rate decay
- Gradient clipping
- Early stopping
- Two-stage fine-tuning (freeze then unfreeze)
- Per-language validation diagnostics

## Model Architecture (Vertical)

```mermaid
flowchart TB
    M1["Input: Premise and Hypothesis Text"] --> M2["XLM-R Tokenizer"]
    M2 --> M3["input_ids and attention_mask"]
    M3 --> M4["XLM-RoBERTa Backbone"]
    M4 --> M5["Token Embeddings"]
    M5 --> M6["Masked Mean Pooling"]
    M6 --> M7["Dropout"]
    M7 --> M8["Dense Softmax 3-Class Head"]
    M8 --> M9["Entailment / Neutral / Contradiction Probabilities"]
```

## How to Use

1. Open `nli_merged_enhanced.ipynb` in Kaggle or Jupyter.
2. Ensure the Kaggle dataset is attached at `../input/contradictory-my-dear-watson/`.
3. Run all cells in order.
4. After training and inference, collect `submission.csv` generated by the notebook.
5. Submit `submission.csv` to the Kaggle competition.

## Output

- `submission.csv` with columns:
  - `id`
  - `prediction`
